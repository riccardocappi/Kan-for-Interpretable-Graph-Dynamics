{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c38417",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24df1505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcappi/.conda/envs/my_env/lib/python3.12/site-packages/juliacall/__init__.py:61: UserWarning: torch was imported before juliacall. This may cause a segfault. To avoid this, import juliacall before importing torch. For updates, see https://github.com/pytorch/pytorch/issues/78829.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.kan.KAN import KAN\n",
    "from utils.utils import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59e2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset according to the specified function\n",
    "def create_dataset(fn, device='cuda'):\n",
    "    x1 = torch.linspace(-1, 1, steps=40)\n",
    "    x2 = torch.linspace(-1, 1, steps=40)\n",
    "    x1, x2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    x_eval = torch.stack([x1.flatten(), x2.flatten()], dim=1)\n",
    "    y_target = fn(x_eval) #+ torch.randn((x_eval.shape[0], 1))*0.1\n",
    "\n",
    "    # test and train split\n",
    "    n_training = int(x_eval.shape[0] * 0.8)\n",
    "    training_idxs = np.random.randint(x_eval.shape[0], size=n_training)\n",
    "    test_idxs = [i for i in range(x_eval.shape[0]) if i not in training_idxs]\n",
    "\n",
    "    x_training = x_eval[training_idxs]\n",
    "    x_test = x_eval[test_idxs]\n",
    "    y_training = y_target[training_idxs]\n",
    "    y_test = y_target[test_idxs]\n",
    "\n",
    "    shuffled_idxs = torch.randperm(x_training.shape[0])\n",
    "    x_training = x_training[shuffled_idxs]\n",
    "    y_training = y_training[shuffled_idxs]\n",
    "\n",
    "    return x_training.to(device), x_test.to(device), y_training.to(device), y_test.to(device)\n",
    "\n",
    "\n",
    "# Evaluates a model on the validation/test set\n",
    "def eval_model(model, x_val, y_val, loss):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(x_val)\n",
    "        val_loss = loss(val_out, y_val)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Trains the model\n",
    "def training(model:KAN, x_training, x_test, y_training, y_test, epochs=1000, patience=50, log=100, lr=0.001, lamb=0.01):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_training)\n",
    "        train_loss = criterion(y_pred, y_training)\n",
    "        reg, _, _ = model.regularization_loss()\n",
    "        loss = train_loss + lamb * reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_loss = eval_model(model, x_test, y_test, criterion)\n",
    "\n",
    "        if epoch % log == 0:\n",
    "            print(f\"Epoch: {epoch} \\t Train Loss: {train_loss: 0.5f} \\t Val Loss: {val_loss: 0.5f} \\t Best Val Loss: {best_val_loss: 0.5f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif epoch - best_epoch >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    return best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "733bb4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train Loss:  0.66161 \t Val Loss:  0.61285 \t Best Val Loss:  inf\n",
      "Epoch: 100 \t Train Loss:  0.00040 \t Val Loss:  0.00041 \t Best Val Loss:  0.00043\n",
      "Epoch: 200 \t Train Loss:  0.00016 \t Val Loss:  0.00016 \t Best Val Loss:  0.00014\n",
      "Epoch: 300 \t Train Loss:  0.00011 \t Val Loss:  0.00010 \t Best Val Loss:  0.00010\n",
      "Epoch: 400 \t Train Loss:  0.00010 \t Val Loss:  0.00009 \t Best Val Loss:  0.00008\n",
      "Epoch: 500 \t Train Loss:  0.00010 \t Val Loss:  0.00006 \t Best Val Loss:  0.00004\n",
      "Epoch: 600 \t Train Loss:  0.00005 \t Val Loss:  0.00007 \t Best Val Loss:  0.00003\n",
      "Epoch: 700 \t Train Loss:  0.00007 \t Val Loss:  0.00008 \t Best Val Loss:  0.00002\n",
      "Epoch: 800 \t Train Loss:  0.00006 \t Val Loss:  0.00005 \t Best Val Loss:  0.00002\n",
      "Epoch: 900 \t Train Loss:  0.00005 \t Val Loss:  0.00006 \t Best Val Loss:  0.00002\n",
      "Epoch: 1000 \t Train Loss:  0.00003 \t Val Loss:  0.00003 \t Best Val Loss:  0.00002\n",
      "Epoch: 1100 \t Train Loss:  0.00006 \t Val Loss:  0.00004 \t Best Val Loss:  0.00002\n",
      "Early stopping at epoch 1196\n"
     ]
    }
   ],
   "source": [
    "model = KAN(\n",
    "    [2, 5, 1],\n",
    "    mu_1=1.,\n",
    "    mu_2=1.,\n",
    "    use_orig_reg=True,\n",
    "    store_act=True\n",
    ")\n",
    "\n",
    "fn = lambda x: (x[:, 0]**2).unsqueeze(-1) + (x[:, 1]).unsqueeze(-1)\n",
    "# fn = lambda x: (x[:, 0]**2 + x[:, 1]**3).unsqueeze(-1)\n",
    "\n",
    "# fn = lambda x: torch.exp(torch.sin(torch.pi * x[:, :1]) + torch.pow(x[:, 1:] + 0.5, 2))\n",
    "x_training, x_test, y_training, y_test = create_dataset(fn)\n",
    "\n",
    "# Training without fixing symbolic functions\n",
    "best_model = training(model, x_training, x_test, y_training, y_test, epochs=3000, patience=500, lr=0.01, lamb=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee014774",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    folder_path='./saved_models_optuna/example_KAN/figures',\n",
    "    layers=model.layers,\n",
    "    show_plots=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c082bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_acts(\n",
    "    layers=model.layers,\n",
    "    folder_path='./saved_models_optuna/example_KAN/cached_acts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55808702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning node (0,0)\n",
      "Pruning node (0,2)\n",
      "Pruning node (0,3)\n",
      "Pruning node (0,4)\n"
     ]
    }
   ],
   "source": [
    "cache_acts, cache_preacts = get_kan_arch(n_layers=2, model_path='./saved_models_optuna/example_KAN')\n",
    "pruned_acts, pruned_preacts = pruning(cache_acts, cache_preacts, theta=0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a22558",
   "metadata": {},
   "outputs": [],
   "source": [
    "symb_g = fit_kan(\n",
    "    pruned_acts,\n",
    "    pruned_preacts,\n",
    "    symb_xs=[sp.Symbol('x_1'), sp.Symbol('x_2')],\n",
    "    model_path='./saved_models_optuna/example_KAN'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4f2ba2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.99 x_{1}^{2} + 0.99 x_{2}$"
      ],
      "text/plain": [
       "0.99*x_1**2 + 0.99*x_2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantise(symb_g[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
